import os
import json
import faiss
import numpy as np
import torch
import gradio as gr
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
INDEX_DIR = "data/index"
EMBED_MODEL_NAME = "BAAI/bge-m3"
LLM_ID = "Qwen/Qwen1.5-7B-Chat"
LORA_DIR = "checkpoints/qwen_lora"
embed_model = SentenceTransformer(EMBED_MODEL_NAME)
index = faiss.read_index(os.path.join(INDEX_DIR, "faiss.index"))
with open(os.path.join(INDEX_DIR, "meta.json"), 'r', encoding='utf-8') as f:
    documents = json.load(f)
tokenizer = AutoTokenizer.from_pretrained(LLM_ID, trust_remote_code=True)
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)
base_model = AutoModelForCausalLM.from_pretrained(LLM_ID, device_map="auto", quantization_config=quantization_config)
llm = PeftModel.from_pretrained(base_model, LORA_DIR)
def rag_inference(question, top_k, temperature):
    q_emb = np.array(embed_model.encode([question], normalize_embeddings=True)).astype('float32')
    D, I = index.search(q_emb, top_k) 
    retrieved_content = ""
    source_list = []
    for i, idx in enumerate(I[0]):
        doc_text = documents[idx]['text'] if isinstance(documents[idx], dict) else documents[idx]
        source_list.append(f"ã€å‚è€ƒèµ„æ–™ {i+1}ã€‘\n{doc_text}")
        retrieved_content += doc_text + "\n---\n"
    prompt = f"### Instruction:\n{question}\n\n### Input:\n{retrieved_content}\n\n### Response:\n"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = llm.generate(
            **inputs, 
            max_new_tokens=512, 
            temperature=temperature,
            do_sample=True if temperature > 0 else False,
            pad_token_id=tokenizer.eos_token_id
        )
    full_res = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = full_res.split("### Response:\n")[-1].strip()
    return answer, "\n\n".join(source_list)-
with gr.Blocks(theme=gr.themes.Soft(), title="RAG Optimization Lab") as demo:
    gr.Markdown("# ğŸ¤– RAG + SFT å‚ç›´é¢†åŸŸæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    gr.Markdown("æœ¬é¡¹ç›®ç”± Qwen-7B-Chat + LoRA å¾®è°ƒ + BGE-M3 æ£€ç´¢å¢å¼ºé©±åŠ¨ã€‚")
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### âš™ï¸ æ§åˆ¶é¢æ¿")
            top_k_slider = gr.Slider(minimum=1, maximum=10, value=3, step=1, label="æ£€ç´¢å‚è€ƒæ¡æ•° (Top-K)")
            temp_slider = gr.Slider(minimum=0.0, maximum=1.0, value=0.1, step=0.05, label="ç”Ÿæˆéšæœºåº¦ (Temperature)")
            btn = gr.Button("ğŸš€ æäº¤æé—®", variant="primary")    
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’¬ å¯¹è¯çª—å£")
            question_input = gr.Textbox(label="è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šè‹¹æœæ˜¯ä»€ä¹ˆï¼Ÿ", lines=2)
            answer_output = gr.Textbox(label="Qwen çš„å›ç­”", interactive=False, lines=10)        
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ“š æ£€ç´¢è¯æ®é“¾")
            sources_output = gr.Textbox(label="æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™åŸæ–‡", interactive=False, lines=15)
    btn.click(
        fn=rag_inference, 
        inputs=[question_input, top_k_slider, temp_slider], 
        outputs=[answer_output, sources_output]
    )
if __name__ == "__main__":
    demo.launch(share=True, server_name="0.0.0.0")