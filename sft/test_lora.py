import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# --- é…ç½® ---
MODEL_ID = "Qwen/Qwen1.5-7B-Chat"
LORA_DIR = "checkpoints/qwen_lora"

def main():
    print("ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

    print("ğŸ§  æ­£åœ¨åŠ è½½ 4-bit åº•åº§æ¨¡å‹...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        quantization_config=quantization_config,
        trust_remote_code=True
    )

    print("âœ¨ æ­£åœ¨æ³¨å…¥ä½ è®­ç»ƒçš„ LoRA çµé­‚...")
    model = PeftModel.from_pretrained(base_model, LORA_DIR)
    
    print("\nâœ… åŠ è½½å®Œæˆï¼å¼€å§‹æµ‹è¯•...")

    # æˆ‘ä»¬æ‹¿ä½ åˆšæ‰é‚£æ¡ Virgin Australia çš„æ•°æ®æ¥è€ƒè€ƒå®ƒ
    instruction = "When did Virgin Australia start operating?"
    context = "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route."

    # ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ—¶çš„æ ¼å¼æ‹¼è£… prompt
    prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{context}\n\n### Response:\n"

    print("\n" + "="*40)
    print("ã€æˆ‘ä»¬çš„æç¤ºè¯ (Prompt)ã€‘:")
    print(prompt)
    print("="*40)

    # æ¨ç†ç”Ÿæˆ
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    # è®©æ¨¡å‹æœ€å¤šç”Ÿæˆ 50 ä¸ª token çš„å›ç­”
    outputs = model.generate(
        **inputs, 
        max_new_tokens=50,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # è§£ç å¹¶æå–æ¨¡å‹çš„çº¯å‡€å›ç­”
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    final_answer = response_text.split("### Response:\n")[-1]

    print("\nğŸ¤– ã€Qwen-LoRA çš„å›ç­”ã€‘:")
    print(final_answer)

if __name__ == "__main__":
    main()