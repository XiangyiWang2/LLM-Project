import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType

# --- 1. é…ç½®å‚æ•° ---
MODEL_ID = "Qwen/Qwen1.5-7B-Chat"
DATA_PATH = "data/raw/dolly15k.jsonl"  # ğŸ‘ˆ å®Œç¾ï¼ç›´æ¥ç”¨ä½ æœ¬åœ°çš„æ•°æ®
OUTPUT_DIR = "checkpoints/qwen_lora"

# LoRA æ ¸å¿ƒå‚æ•°
LORA_R = 8
LORA_ALPHA = 32
LORA_DROPOUT = 0.1

def format_dolly(sample):
    """å°† Alpaca æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸º Qwen è®¤è¯†çš„æŒ‡ä»¤æ ¼å¼"""
    
    # 1. æ‹¿æŒ‡ä»¤
    instruction = sample.get('instruction', '')
    
    # 2. æ‹¿å‚è€ƒèµ„æ–™ (ä½ çš„æ•°æ®é‡Œå« 'input')
    context = sample.get('input', '')
    
    # 3. æ‹¿ç­”æ¡ˆ (ä½ çš„æ•°æ®é‡Œå« 'output')
    response = sample.get('output', '')
    
    # 4. æ‹¼è£…æˆ Qwen å–œæ¬¢çš„æ ¼å¼
    prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{context}\n\n### Response:\n"
    
    return {"text": prompt + response + "<|im_end|>"}
def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_ID}...")
    
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    # 2. 4-bit é‡åŒ–é…ç½® (æå…¶çœæ˜¾å­˜çš„å…³é”®)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4"
    )

    # 3. åŠ è½½åº•åº§æ¨¡å‹
    print("ğŸ§  æ­£åœ¨åŠ è½½ 4-bit åº•åº§æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        quantization_config=quantization_config,
        trust_remote_code=True
    )

    # 4. é…ç½®å¹¶æŒ‚è½½ LoRA é€‚é…å™¨
    print("ğŸ› ï¸ æ­£åœ¨æŒ‚è½½ LoRA é€‚é…å™¨...")
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, 
        inference_mode=False, 
        r=LORA_R, 
        lora_alpha=LORA_ALPHA, 
        lora_dropout=LORA_DROPOUT,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
    )
    model = get_peft_model(model, peft_config)
    
    # ğŸŒŸ è§è¯å¥‡è¿¹çš„æ—¶åˆ»
    model.print_trainable_parameters() 

    # 5. åŠ è½½æœ¬åœ°æ•°æ®
    print(f"ğŸ“š æ­£åœ¨åŠ è½½æœ¬åœ°æ•°æ®é›†: {DATA_PATH}...")
    dataset = load_dataset("json", data_files=DATA_PATH, split="train")
    
    # ã€å®éªŒé˜¶æ®µä¸“ç”¨ã€‘ï¼šåªå–å‰ 1000 æ¡æ•°æ®å¿«é€Ÿè·‘é€šé—­ç¯ï¼
    print("âš¡ æŠ½å–å‰ 1000 æ¡æ•°æ®è¿›è¡Œå¿«é€ŸéªŒè¯...")
    dataset = dataset.select(range(1000)) 
    
    dataset = dataset.map(format_dolly)
    
    def process_func(example):
        tokenized = tokenizer(example["text"], truncation=True, max_length=512, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    print("âœ‚ï¸ æ­£åœ¨ Tokenize æ•°æ®...")
    tokenized_ds = dataset.map(process_func, remove_columns=dataset.column_names)

    # 6. è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=2, # å¦‚æœæ˜¾å­˜ OOMï¼Œæ”¹å› 1
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        num_train_epochs=1,
        logging_steps=10,
        save_strategy="epoch",
        optim="paged_adamw_32bit",
        fp16=True,
        remove_unused_columns=False
    )

    # 7. å¯åŠ¨ Trainer
    print("ğŸ”¥ å¼€å§‹ SFT å¾®è°ƒ...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_ds,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    )

    trainer.train()
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼LoRA æƒé‡å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)

if __name__ == "__main__":
    main()